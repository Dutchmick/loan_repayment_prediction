{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Predict Kaggle competition home credit default risk\n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make root folder the current working directory\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_folder = './data/processed/basic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = './reports/figures/basic/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective of creating a model that can be applied to the financially excluded, I deselected columns with data related to credit bureau, bank related data and formal loans.\n",
    "\n",
    "The data dictionary can be found here:\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data?select=HomeCredit_columns_description.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns which can be used by financially excluded\n",
    "column_selection = ['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE',\n",
    "                    'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "                    'CNT_CHILDREN', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
    "                    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',\n",
    "                    'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE',\n",
    "                    'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
    "                    'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL',\n",
    "                    'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n",
    "                    'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE',\n",
    "                    'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n",
    "                    'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START',\n",
    "                    'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',\n",
    "                    'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n",
    "                    'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',\n",
    "                    'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',\n",
    "                    'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n",
    "                    'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n",
    "                    'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n",
    "                    'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG',\n",
    "                    'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG',\n",
    "                    'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n",
    "                    'APARTMENTS_MODE', 'BASEMENTAREA_MODE',\n",
    "                    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n",
    "                    'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE',\n",
    "                    'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n",
    "                    'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
    "                    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE',\n",
    "                    'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n",
    "                    'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI',\n",
    "                    'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI',\n",
    "                    'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n",
    "                    'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n",
    "                    'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
    "                    'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE',\n",
    "                    'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\n",
    "                    'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "                    'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "                    'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2',\n",
    "                    'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "                    'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n",
    "                    'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
    "                    'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n",
    "                    'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "                    'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
    "                    'FLAG_DOCUMENT_21', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',\n",
    "                    'AMT_ANNUITY', 'AMT_GOODS_PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile(input_folder + 'home-credit-default-risk.zip')\n",
    "application_train_df = pd.read_csv(zip_file.open('application_train.csv'), usecols=column_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outcome column from test data\n",
    "column_selection.remove('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile(input_folder + 'home-credit-default-risk.zip')\n",
    "application_test_df = pd.read_csv(zip_file.open('application_test.csv'), usecols=column_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile(input_folder + 'home-credit-default-risk.zip')\n",
    "credit_card_df = pd.read_csv(zip_file.open('credit_card_balance.csv'), usecols=('SK_ID_CURR', 'NAME_CONTRACT_STATUS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = ZipFile(input_folder + 'home-credit-default-risk.zip')\n",
    "bureau_df = pd.read_csv(zip_file.open('bureau.csv'), usecols=('SK_ID_CURR', 'CREDIT_ACTIVE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select financially excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to select unbanked customers to later evaluate that the ML model doesn't classify them as repayment risk. In the dataset there is no data available on whether the customer has a bank account, which is usually the first formal financial product people start using. Instead, we will be classifying someone as unbanked if there is no credit card or credit bureau information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_fin_included_df = list(credit_card_df['SK_ID_CURR']) + list(bureau_df['SK_ID_CURR'])\n",
    "ID_fin_included_df = list(dict.fromkeys(ID_fin_included_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_all_applicants = application_train_df['SK_ID_CURR']\n",
    "ID_fin_excluded = list(set(list(ID_all_applicants)) - set(ID_fin_included_df))\n",
    "ID_fin_excluded_df = pd.DataFrame(ID_fin_excluded, columns=['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test and train data for cleaning with column to distinguish\n",
    "application_test_df['train'] = 0\n",
    "application_train_df['train'] = 1\n",
    "application_df = pd.concat([application_train_df, application_test_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test_df = application_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns which can be used for financially excluded customers\n",
    "column_selection = ['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE',\n",
    "                    'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "                    'CNT_CHILDREN', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
    "                    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',\n",
    "                    'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE',\n",
    "                    'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
    "                    'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL',\n",
    "                    'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n",
    "                    'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE',\n",
    "                    'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n",
    "                    'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START',\n",
    "                    'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',\n",
    "                    'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n",
    "                    'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',\n",
    "                    'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',\n",
    "                    'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n",
    "                    'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n",
    "                    'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n",
    "                    'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG',\n",
    "                    'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG',\n",
    "                    'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n",
    "                    'APARTMENTS_MODE', 'BASEMENTAREA_MODE',\n",
    "                    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n",
    "                    'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE',\n",
    "                    'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n",
    "                    'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n",
    "                    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE',\n",
    "                    'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n",
    "                    'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI',\n",
    "                    'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI',\n",
    "                    'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n",
    "                    'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n",
    "                    'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n",
    "                    'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE',\n",
    "                    'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\n",
    "                    'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "                    'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "                    'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2',\n",
    "                    'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "                    'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n",
    "                    'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
    "                    'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n",
    "                    'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "                    'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
    "                    'FLAG_DOCUMENT_21', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',\n",
    "                    'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'train']\n",
    "\n",
    "application_df = application_df[column_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform variables as per EDA\n",
    "application_df['AMT_INCOME_TOTAL_log'] = np.log(application_df['AMT_INCOME_TOTAL'])\n",
    "application_df['AMT_CREDIT_log'] = np.log(application_df['AMT_CREDIT'])\n",
    "application_df['AMT_ANNUITY_log'] = np.log(application_df['AMT_ANNUITY'])\n",
    "application_df['AMT_GOODS_PRICE_log'] = np.log(application_df['AMT_GOODS_PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make negative values positive in preparation of log transformation\n",
    "# Remove zero values by adding 1\n",
    "application_df['DAYS_REGISTRATION'] = application_df['DAYS_REGISTRATION'] * -1 + 1\n",
    "application_df['DAYS_REGISTRATION_log'] = np.log(application_df['DAYS_REGISTRATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop log transformed variables\n",
    "cols_to_drop = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_REGISTRATION']\n",
    "application_df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = list(application_df.select_dtypes(include=['float64']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df = application_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode organisation subcategories into one main category (eg 'Trade: type 6 -> 'Trade)\n",
    "column = application_df['ORGANIZATION_TYPE']\n",
    "types = ['Trade', 'Industry', 'Transport', 'Business']\n",
    "\n",
    "for item in types:\n",
    "    column.loc[column.str.startswith(item, na=False)] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Other' category for categorical outliers\n",
    "threshold = 0.01\n",
    "dataframe = application_df\n",
    "for col in dataframe[['ORGANIZATION_TYPE', 'OCCUPATION_TYPE', 'NAME_INCOME_TYPE']]:\n",
    "    value_counts = dataframe[col].value_counts(normalize=True)\n",
    "    below_threshold = value_counts[value_counts <= threshold].index.tolist()\n",
    "    dataframe[col].replace(to_replace=below_threshold, value=\"Other2\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with outlier categorical values\n",
    "column_selection = list(application_df.select_dtypes(include=['object']))\n",
    "threshold_percent = 0.01\n",
    "\n",
    "for columns in column_selection:\n",
    "    # Select rows falling below threshold\n",
    "    rows_below_threshold = application_df[application_df[columns].map(application_df[columns].value_counts(normalize=True, dropna=False) < threshold_percent)]\n",
    "    if len(rows_below_threshold.index) > 0:\n",
    "        application_df.drop(rows_below_threshold.index, inplace=True)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with 50% or more missing data\n",
    "missing_data = application_df.isna().sum() / application_df.isna().count() * 100\n",
    "features_missing = list(missing_data[missing_data > 50].keys())\n",
    "application_df.drop(features_missing, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'XNA' with the most frequent occuring gender response\n",
    "freq = application_df['CODE_GENDER'].value_counts().index.tolist()[0]\n",
    "application_df['CODE_GENDER'].replace('XNA', freq, inplace=False)\n",
    "application_df['CODE_GENDER'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the missing data % in training data\n",
    "missing = application_df.isna().sum() / application_df.isna().count() * 100\n",
    "missing = missing[missing > 0]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cases with between 0-1% of missing data\n",
    "minimal_missing = missing[(missing > 0) & (missing < 1)]\n",
    "minimal_missing = list(minimal_missing.index)\n",
    "application_df.dropna(axis=0, subset=minimal_missing, inplace=True)\n",
    "print(\"Cases from the following columns were dropped:\", minimal_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(application_df.columns[application_df.isna().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find way to handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the missing values of continous features\n",
    "num_columns = list(application_df.select_dtypes(include=['float64']).columns)\n",
    "df = application_df[num_columns].copy()\n",
    "\n",
    "cols_nan = list(df.columns[df.isna().any()])\n",
    "cols_no_nan = list(df.columns.difference(cols_nan).values)\n",
    "\n",
    "cols_nan.remove('TARGET')\n",
    "\n",
    "for col in cols_nan:\n",
    "    test_data = df[df[col].isna()]\n",
    "    train_data = df.dropna()\n",
    "    model = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "    application_df.loc[application_df[col].isna(), col] = model.predict(test_data[cols_no_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_df.copy()\n",
    "\n",
    "cols_nan = list(df.columns[df.isna().any()])\n",
    "cols_no_nan = df.columns.difference(cols_nan).values\n",
    "cat_var = list(df.select_dtypes(include=['O', 'object', 'datetime64']).columns)\n",
    "cols_no_nan = [x for x in cols_no_nan if x not in cat_var]\n",
    "\n",
    "cols_no_nan.remove('SK_ID_CURR')\n",
    "cols_nan.remove('TARGET')\n",
    "\n",
    "for col in cols_nan:\n",
    "    test_data = df[df[col].isna()]\n",
    "    train_data = df.dropna()\n",
    "    model = KNeighborsClassifier(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n",
    "    application_df.loc[application_df[col].isna(), col] = model.predict(test_data[cols_no_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining missing data % in training data\n",
    "train_missing_data = application_df.isna().sum() / application_df.isna().count() * 100\n",
    "train_missing_data[train_missing_data > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummify categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify categorical features\n",
    "column_selection = list(application_df.select_dtypes(include=['object']))\n",
    "# Create dummies of categorical features\n",
    "application_df = pd.get_dummies(application_df, columns=column_selection, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature loan % of  income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df = application_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df['creditVSincome'] = application_df['AMT_CREDIT'] / application_df['AMT_INCOME_TOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best performing features\n",
    "feature_selection = list(feature_scores_df[:20]['features'])\n",
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absCorrWithDep = []\n",
    "\n",
    "for var in features:\n",
    "    absCorrWithDep.append(abs(y.corr(train_df[var])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores_df = pd.DataFrame()\n",
    "feature_scores_df['features'] = features\n",
    "feature_scores_df['scores'] = absCorrWithDep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores_df = feature_scores_df.sort_values(by='scores', ascending=False)\n",
    "feature_scores_df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores_df.drop_duplicates(subset=['scores'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.04\n",
    "feature_selection = list(feature_scores_df[feature_scores_df['scores'] > threshold]['features'])\n",
    "len(feature_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-collineairity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Variation Inflation factor (VIF)\n",
    "X = application_df[feature_selection]\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_corr = pd.merge(feature_scores_df, vif, how='left', on='features')\n",
    "vif_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop_1 = ['DAYS_BIRTH_FLAG_MOBIL', 'FLAG_MOBIL_DAYS_LAST_PHONE_CHANGE', 'FLAG_MOBIL_REGION_RATING_CLIENT_W_CITY',\n",
    "                      'DAYS_ID_PUBLISH_FLAG_MOBIL', 'FLAG_MOBIL_REGION_RATING_CLIENT', 'FLAG_MOBIL_REG_CITY_NOT_WORK_CITY',\n",
    "                      'REGION_RATING_CLIENT_W_CITY_REG_CITY_NOT_LIVE_CITY', 'DAYS_BIRTH_FLAG_CONT_MOBILE',\n",
    "                      'REGION_RATING_CLIENT_W_CITY_REG_CITY_NOT_WORK_CITY', 'REGION_RATING_CLIENT_FLAG_DOCUMENT_3', 'FLAG_MOBIL_DAYS_LAST_PHONE_CHANGE',\n",
    "                      'FLAG_CONT_MOBILE_REGION_RATING_CLIENT_W_CITY', 'DAYS_ID_PUBLISH_FLAG_CONT_MOBILE', 'FLAG_CONT_MOBILE_REGION_RATING_CLIENT',\n",
    "                      'FLAG_MOBIL_REG_CITY_NOT_WORK_CITY', 'REGION_RATING_CLIENT_REG_CITY_NOT_LIVE_CITY', 'FLAG_EMP_PHONE_REG_CITY_NOT_WORK_CITY',\n",
    "                      'DAYS_BIRTH', 'FLAG_MOBIL_REG_CITY_NOT_WORK_CITY', 'FLAG_EMP_PHONE_REGION_RATING_CLIENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly collineair features\n",
    "application_df.drop(columns=features_to_drop_1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = list(set(feature_selection) - set(features_to_drop_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Variation Inflation factor (VIF)\n",
    "X = application_df[feature_selection]\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_corr = pd.merge(feature_scores_df, vif, how='left', on='features')\n",
    "vif_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features below multi-collineairity threshold (VIF >10)\n",
    "multi_col = list(vif[vif['VIF Factor'] < 10]['features'])\n",
    "multi_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all required columns\n",
    "no_feat = ['TARGET', 'SK_ID_CURR', 'train']\n",
    "column_selection = multi_col + no_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df = application_df[column_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test and train dataset\n",
    "train_df = application_df[application_df['train'] == 1].copy()\n",
    "test_df = application_df[application_df['train'] == 0].copy()\n",
    "\n",
    "train_df.drop(columns=['train'], inplace=True)\n",
    "test_df.drop(columns=['train'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictor and dependent variables\n",
    "predictor = 'TARGET'\n",
    "Y_train = train_df[predictor]\n",
    "X_train = train_df.drop(predictor, axis=1)\n",
    "test_df.drop(columns=predictor, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy array of SK_ID_CURR to later use for submission file\n",
    "test_ids_df = test_df['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column\n",
    "test_df.drop(columns='SK_ID_CURR', inplace=True)\n",
    "X_train.drop(columns='SK_ID_CURR', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the features\n",
    "ss = StandardScaler()\n",
    "x_train = pd.DataFrame(ss.fit_transform(x_train), columns=x_train.columns)\n",
    "x_test = pd.DataFrame(ss.fit_transform(x_test), columns=x_test.columns)\n",
    "test_df = pd.DataFrame(ss.fit_transform(test_df), columns=test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to cleaned data folder\n",
    "x_train.to_csv(cleaned_folder + 'x_train.csv', index=False)\n",
    "x_test.to_csv(cleaned_folder + 'x_test.csv', index=False)\n",
    "y_train.to_csv(cleaned_folder + 'y_train.csv', index=False)\n",
    "y_test.to_csv(cleaned_folder + 'y_test.csv', index=False)\n",
    "test_df.to_csv(cleaned_folder + 'test_df.csv', index=False)\n",
    "test_ids_df.to_csv(cleaned_folder + 'test_ids_df.csv', index=False)\n",
    "ID_fin_excluded_df.to_csv(cleaned_folder + 'IDs_fin_excluded.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
