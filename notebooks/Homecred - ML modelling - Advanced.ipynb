{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentential improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use unbalanced dataset vs artificially balanced\n",
    "- Use a different method of feature selection (*e.g. correlation, Recursive feature selection*) \n",
    "- Include more or different algorithms\n",
    "- Include more parameters as part of the Hyper Parameter Tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics as met\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier, KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from scipy.stats import randint\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "output_folder = './reports/figures/'\n",
    "cleaned_folder = './data/processed/'\n",
    "external_data = './data/external/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:37: E226 missing whitespace around arithmetic operator\n",
      "2:36: E226 missing whitespace around arithmetic operator\n",
      "3:37: E226 missing whitespace around arithmetic operator\n",
      "4:36: E226 missing whitespace around arithmetic operator\n",
      "5:37: E226 missing whitespace around arithmetic operator\n",
      "6:41: E226 missing whitespace around arithmetic operator\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv(cleaned_folder+'x_train.csv')\n",
    "x_test = pd.read_csv(cleaned_folder+'x_test.csv')\n",
    "y_train = pd.read_csv(cleaned_folder+'y_train.csv', dtype='int64')\n",
    "y_test = pd.read_csv(cleaned_folder+'y_test.csv', dtype='int64')\n",
    "test_df = pd.read_csv(cleaned_folder+'test_df.csv')\n",
    "test_ids_df = pd.read_csv(cleaned_folder+'test_ids_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test['TARGET']\n",
    "y_train = y_train['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    168267\n",
       "1     15238\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the dataset is balanced\n",
    "y_train.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: dataset unbalanced with only 9% customers with payment difficulties, which can be expected.\n",
    "\n",
    "*While some algorithms can work with unbalanced datasets, we will be balancing the dataset to allow for algorithms that require a balanced sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcome variable & features\n",
    "train_df = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset based on outcome variable\n",
    "no_pay_prob = train_df[train_df['TARGET'] == 0]\n",
    "pay_prob = train_df[train_df['TARGET'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample - artificially add customers with payment difficulties\n",
    "# Reason for upsampling is that our dataset is relatively small\n",
    "pay_prob2 = resample(pay_prob,\n",
    "                     replace=True,  # sample with replacement\n",
    "                     n_samples=len(no_pay_prob),  # dataset to match customers without payment problems\n",
    "                     random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15238, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of customers with payment difficulties\n",
    "pay_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168267, 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New count of customers without payment difficulties\n",
    "pay_prob2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataset with added cases\n",
    "train_df = pd.concat([pay_prob2, no_pay_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset in preparation of modelling\n",
    "y_train = train_df['TARGET']\n",
    "x_train = train_df.drop('TARGET', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 selected features\n"
     ]
    }
   ],
   "source": [
    "rf_feature_select = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "rf_feature_select.fit(x_train, y_train)\n",
    "\n",
    "rf_sel_feature_count = rf_feature_select.get_support()\n",
    "rf_selected_features = x_train.loc[:, rf_sel_feature_count].columns.tolist()\n",
    "print(str(len(rf_selected_features)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['REGION_POPULATION_RELATIVE_DAYS_REGISTRATION', 'DAYS_REGISTRATION_DAYS_LAST_PHONE_CHANGE', 'DAYS_LAST_PHONE_CHANGE_AMT_ANNUITY', 'REGION_POPULATION_RELATIVE_DAYS_EMPLOYED', 'DAYS_EMPLOYED_AMT_INCOME_TOTAL', 'DAYS_BIRTH_AMT_INCOME_TOTAL', 'DAYS_REGISTRATION_AMT_GOODS_PRICE', 'DAYS_EMPLOYED_CNT_FAM_MEMBERS', 'DAYS_ID_PUBLISH_DAYS_LAST_PHONE_CHANGE', 'DAYS_ID_PUBLISH_AMT_GOODS_PRICE', 'CNT_FAM_MEMBERS_DAYS_LAST_PHONE_CHANGE', 'REGION_POPULATION_RELATIVE_DAYS_ID_PUBLISH', 'DAYS_EMPLOYED_AMT_ANNUITY', 'DAYS_EMPLOYED_DAYS_LAST_PHONE_CHANGE', 'DAYS_EMPLOYED_DAYS_REGISTRATION', 'REGION_POPULATION_RELATIVE_DAYS_LAST_PHONE_CHANGE']\n"
     ]
    }
   ],
   "source": [
    "print('Selected features:', rf_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select strongest features\n",
    "x_train = x_train[rf_selected_features]\n",
    "x_test = x_test[rf_selected_features]\n",
    "test_df = test_df[rf_selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'KNN': KNeighborsClassifier(),\n",
    "               'Random Forest Classifier': RandomForestClassifier(),\n",
    "               'Decision Tree Classifier': DecisionTreeClassifier(),\n",
    "               'Logistic Regression': LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8:5: E122 continuation line missing indentation or outdented\n",
      "9:5: E122 continuation line missing indentation or outdented\n"
     ]
    }
   ],
   "source": [
    "# Evaluate strongest predicting algorithm in default setting\n",
    "base_score = 0\n",
    "model_outcomes = []\n",
    "for Name, classify in classifiers.items():\n",
    "    classify.fit(x_train, y_train)\n",
    "    predicting_y = classify.predict(x_test)\n",
    "    model_outcomes.append({\n",
    "                           'Algorithm': str(Name),\n",
    "                           'f1_score': str(met.f1_score(y_test, predicting_y))})\n",
    "    if met.f1_score(y_test, predicting_y) > base_score:\n",
    "        # prediction = classify.predict(test_df)\n",
    "        base_score = met.f1_score(y_test, predicting_y)\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.1891646414807178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.18504152596513646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.17854214319623665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear Discriminant Analyis</td>\n",
       "      <td>0.1782727409541419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.17361925955897228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.17095610446550324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.14415359173960393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.09675179384306766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.00031055900621118014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Algorithm                f1_score\n",
       "0  Gradient Boosting Classifier      0.1891646414807178\n",
       "1          Ada Boost Classifier     0.18504152596513646\n",
       "8           Logistic Regression     0.17854214319623665\n",
       "2   Linear Discriminant Analyis      0.1782727409541419\n",
       "4                   BernoulliNB     0.17361925955897228\n",
       "3                    GaussianNB     0.17095610446550324\n",
       "5                           KNN     0.14415359173960393\n",
       "7      Decision Tree Classifier     0.09675179384306766\n",
       "6      Random Forest Classifier  0.00031055900621118014"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame(model_outcomes, columns=['Algorithm', 'f1_score'])\n",
    "model_scores.sort_values(by=['f1_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best algorithms according to model evaluation:\n",
    "<ol>\n",
    "<li> GaussianNB </li>\n",
    "<li> BernoulliNB </li>\n",
    "<li> Linear Discriminant Analyis </li>\n",
    "<li> Logistic regression </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning\n",
    "Optimise the top performing algorithms to create the best possible prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'Random_forest': {'model': RandomForestClassifier(),\n",
    "                                 'params': {'n_estimators': [31, 35, 37]}},\n",
    "               'Logistic_regression': {'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n",
    "                                       'params': {'C': [1, 10, 100, 1000],\n",
    "                                                  'penalty': ['l1', 'l2'], }},\n",
    "               'KNearestNeighbors': {'model': KNeighborsClassifier(),\n",
    "                                     'params': {'n_neighbors': [2, 5, 7],\n",
    "                                                'metric': ['euclidean', 'minkowski']}},\n",
    "               'DecisionTreeClassifier': {'model': DecisionTreeClassifier(),\n",
    "                                          'params': {'criterion': [\"gini\", \"entropy\"],\n",
    "                                                     'splitter': ['best', 'random'],\n",
    "                                                     'max_depth': [3, None],\n",
    "                                                     'max_features': [1, 5, 9],\n",
    "                                                     'min_samples_leaf': [1, 5, 9]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classifier algorithms to optimise\n",
    "scores = []\n",
    "for model_name, mp in classifiers.items():\n",
    "    grid = GridSearchCV(mp['model'],\n",
    "                        mp['params'],\n",
    "                        cv=10,\n",
    "                        scoring='f1_score',\n",
    "                        return_train_score=False,\n",
    "                        n_jobs=-1)\n",
    "    grid.fit(x_train, y_train)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid.best_score_,\n",
    "        'best_params': grid.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with best parameters per algorithm\n",
    "model_parameters = pd.DataFrame(scores, columns=['model',\n",
    "                                                 'best_score',\n",
    "                                                 'best_params'])\n",
    "model_parameters.sort_values(by=['best_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** models perform slightly better than before parameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the earlier identified top 3 algorithms with best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GaussianNB()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BernoulliNB(alpha=0.01)\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearDiscriminantAnalysis(tol=0.0001)\n",
    "model3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LogisticRegression(solver='liblinear',\n",
    "                            multi_class='auto',\n",
    "                            C=1,\n",
    "                            penalty='l1')\n",
    "model4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions from the training data on the test data\n",
    "y_pred_mod_1 = model1.predict(x_test)\n",
    "y_pred_mod_2 = model2.predict(x_test)\n",
    "y_pred_mod_3 = model3.predict(x_test)\n",
    "y_pred_mod_4 = model4.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on training data\n",
    "pred_prob_1 = model1.predict_proba(x_test)\n",
    "pred_prob_2 = model2.predict_proba(x_test)\n",
    "pred_prob_3 = model3.predict_proba(x_test)\n",
    "pred_prob_4 = model4.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve for models\n",
    "fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob_1[:, 1], pos_label=1)\n",
    "fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob_2[:, 1], pos_label=1)\n",
    "fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob_3[:, 1], pos_label=1)\n",
    "fpr4, tpr4, thresh4 = roc_curve(y_test, pred_prob_4[:, 1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark predictions based on random chance for ROC comparison\n",
    "random_probs = [0 for i in range(len(y_test))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate auc scores\n",
    "auc_score1 = roc_auc_score(y_test, pred_prob_1[:, 1])\n",
    "auc_score2 = roc_auc_score(y_test, pred_prob_2[:, 1])\n",
    "auc_score3 = roc_auc_score(y_test, pred_prob_3[:, 1])\n",
    "auc_score4 = roc_auc_score(y_test, pred_prob_4[:, 1])\n",
    "\n",
    "print('Model1 AUC:', auc_score1, 'Model2 AUC:', auc_score2, 'Model3 AUC:', auc_score3, 'Model4 AUC:', auc_score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# plot roc curves\n",
    "plt.plot(fpr1, tpr1, linestyle='--', color='orange', label='GaussianNB')\n",
    "plt.plot(fpr2, tpr2, linestyle='--', color='green', label='BernoulliNB')\n",
    "plt.plot(fpr3, tpr3, linestyle='--', color='purple', label='LinearDiscriminantAnalysis')\n",
    "plt.plot(fpr4, tpr4, linestyle='--', color='blue', label='LogisticRegression')\n",
    "plt.plot(p_fpr, p_tpr, linestyle='--', color='red')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(cleaned_folder+'ROC_graph', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graph, the Gaussia Naive Bayes is the best at predicting customers with payment difficulties, but only slightly better than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "print(\"Confusion matrix model 1\")\n",
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predicted = pd.Series(y_pred_mod_1, name='Predicted')\n",
    "pd.crosstab(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_predicted, y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "print(\"Confusion matrix model 2\")\n",
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predicted = pd.Series(y_pred_mod_2, name='Predicted')\n",
    "pd.crosstab(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_predicted, y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "print(\"Confusion matrix model 3\")\n",
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predicted = pd.Series(y_pred_mod_3, name='Predicted')\n",
    "pd.crosstab(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_predicted, y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "print(\"Confusion matrix model 4\")\n",
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predicted = pd.Series(y_pred_mod_4, name='Predicted')\n",
    "pd.crosstab(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_predicted, y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test datasets\n",
    "x_df = pd.concat([x_train, x_test])\n",
    "y_df = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = GaussianNB()\n",
    "final_model.fit(x_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions based on generated model\n",
    "prediction = final_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Submission File\n",
    "SK_ID_CURR = list(test_ids_df['SK_ID_CURR'])\n",
    "predicted_test_values = pd.DataFrame({'SK_ID_CURR': SK_ID_CURR, 'TARGET': prediction})\n",
    "predicted_test_values.to_csv(external_data + 'Submission_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
